{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Textual Similarity\n",
    "\n",
    "Input is (sentence_A, sentence_B) pairs, with float similarity score as labels. \n",
    "\n",
    "The loss function is CosineSimilarityLoss. \n",
    "\n",
    "This examples trains BERT (or any other transformer model like RoBERTa, DistilBERT etc.) for the STSbenchmark from scratch. It generates sentence embeddings that can be compared using cosine-similarity to measure the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /data02/hyzhang10/pengxia2/cache/officials/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = '/data02/hyzhang10/pengxia2/cache/officials/all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29a95c5c30f4f629c8560347c3fac6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/38.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ec9ab489414592803c6418066c533d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/782k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9503055f8cec435982258e830819162a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/810k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b49d002b9fa43308a825606e56cf776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/557850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42813386346d4086885c4e92115024f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/6584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd098c5e93841a1b72f7248f57ea53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/6609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"sentence-transformers/all-nli\", \"triplet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor': 'A person on a horse jumps over a broken down airplane.',\n",
       " 'positive': 'A person is outdoors, on a horse.',\n",
       " 'negative': 'A person is at a diner, ordering an omelette.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b1a7074b7547a98b5c9217af604654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028d102667574dd292686fc8cbe5b4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/101762 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quora_train_dataset = load_dataset(\"sentence-transformers/quora-duplicates\", \"triplet\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: This script was modified with the v3 release of Sentence Transformers.\n",
    "As a result, it does not produce exactly the same behaviour as the original script.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.similarity_functions import SimilarityFunction\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import (\n",
    "    BatchSamplers,\n",
    "    MultiDatasetBatchSamplers,\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")\n",
    "\n",
    "# Set the log level to INFO to get more information\n",
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO)\n",
    "\n",
    "model_name = \"distilroberta-base\"\n",
    "num_epochs = 1\n",
    "batch_size = 128\n",
    "max_seq_length = 128\n",
    "\n",
    "# Save path of the model\n",
    "output_dir = (\n",
    "    \"output/training_paraphrases_\" + model_name.replace(\"/\", \"-\") + \"-\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    ")\n",
    "\n",
    "# 2. Load some training dataset from: https://huggingface.co/datasets?other=sentence-transformers\n",
    "# Notably, we are looking for datasets compatible with MultipleNegativesRankingLoss, which accepts\n",
    "# triplets of sentences (anchor, positive, negative) and pairs of sentences (anchor, positive).\n",
    "all_nli_train_dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"train\")\n",
    "sentence_compression_train_dataset = load_dataset(\"sentence-transformers/sentence-compression\", split=\"train\")\n",
    "simple_wiki_train_dataset = load_dataset(\"sentence-transformers/simple-wiki\", split=\"train\")\n",
    "altlex_train_dataset = load_dataset(\"sentence-transformers/altlex\", split=\"train\")\n",
    "quora_train_dataset = load_dataset(\"sentence-transformers/quora-duplicates\", \"triplet\", split=\"train\")\n",
    "coco_train_dataset = load_dataset(\"sentence-transformers/coco-captions\", split=\"train\")\n",
    "flickr_train_dataset = load_dataset(\"sentence-transformers/flickr30k-captions\", split=\"train\")\n",
    "yahoo_answers_train_dataset = load_dataset(\n",
    "    \"sentence-transformers/yahoo-answers\", \"title-question-answer-pair\", split=\"train\"\n",
    ")\n",
    "stack_exchange_train_dataset = load_dataset(\n",
    "    \"sentence-transformers/stackexchange-duplicates\", \"title-title-pair\", split=\"train\"\n",
    ")\n",
    "\n",
    "train_dataset_dict = {\n",
    "    \"all-nli\": all_nli_train_dataset,\n",
    "    \"sentence-compression\": sentence_compression_train_dataset,\n",
    "    \"simple-wiki\": simple_wiki_train_dataset,\n",
    "    \"altlex\": altlex_train_dataset,\n",
    "    \"quora-duplicates\": quora_train_dataset,\n",
    "    \"coco-captions\": coco_train_dataset,\n",
    "    \"flickr30k-captions\": flickr_train_dataset,\n",
    "    \"yahoo-answers\": yahoo_answers_train_dataset,\n",
    "    \"stack-exchange\": stack_exchange_train_dataset,\n",
    "}\n",
    "print(train_dataset_dict)\n",
    "\n",
    "# 1. Here we define our SentenceTransformer model. If not already a Sentence Transformer model, it will automatically\n",
    "# create one with \"mean\" pooling.\n",
    "model = SentenceTransformer(model_name)\n",
    "# If we want, we can limit the maximum sequence length for the model\n",
    "model.max_seq_length = max_seq_length\n",
    "logging.info(model)\n",
    "\n",
    "# 3. Define our training loss\n",
    "train_loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# 4. Define an evaluator for use during training. This is useful to keep track of alongside the evaluation loss.\n",
    "stsb_eval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=stsb_eval_dataset[\"sentence1\"],\n",
    "    sentences2=stsb_eval_dataset[\"sentence2\"],\n",
    "    scores=stsb_eval_dataset[\"score\"],\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    "    name=\"sts-dev\",\n",
    ")\n",
    "\n",
    "# 5. Define the training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=output_dir,\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    # We can use ROUND_ROBIN or PROPORTIONAL - to avoid focusing too much on one dataset, we will\n",
    "    # use round robin, which samples the same amount of batches from each dataset, until one dataset is empty\n",
    "    multi_dataset_batch_sampler=MultiDatasetBatchSamplers.ROUND_ROBIN,\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=\"paraphrases-multi\",  # Will be used in W&B if `wandb` is installed\n",
    ")\n",
    "\n",
    "# 6. Create the trainer & start training\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset_dict,\n",
    "    loss=train_loss,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 7. Evaluate the model performance on the STS Benchmark test dataset\n",
    "test_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"test\")\n",
    "test_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=test_dataset[\"sentence1\"],\n",
    "    sentences2=test_dataset[\"sentence2\"],\n",
    "    scores=test_dataset[\"score\"],\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    "    name=\"sts-test\",\n",
    ")\n",
    "test_evaluator(model)\n",
    "\n",
    "# 8. Save the trained & evaluated model locally\n",
    "final_output_dir = f\"{output_dir}/final\"\n",
    "model.save(final_output_dir)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xp-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
