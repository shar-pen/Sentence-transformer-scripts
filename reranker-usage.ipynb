{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Rank\n",
    "\n",
    "Given a search query, we first use a retrieval system that retrieves a large list of e.g. 100 possible hits which are potentially relevant for the query. For the retrieval, we can use either lexical search, e.g. with a vector engine like Elasticsearch, or we can use dense retrieval with a bi-encoder. However, the retrieval system might retrieve documents that are not that relevant for the search query. Hence, in a second stage, we use a re-ranker based on a cross-encoder that scores the relevancy of all candidates for the given search query. The output will be a ranked list of hits we can present to the user.\n",
    "\n",
    "The retriever has to be efficient for large document collections with millions of entries. However, it might return irrelevant candidates. A re-ranker based on a Cross-Encoder can substantially improve the final results for the user. The query and a possible document is passed simultaneously to transformer network, which then outputs a single score between 0 and 1 indicating how relevant the document is for the given query.\n",
    "\n",
    "\n",
    "Characteristics of Cross Encoder (a.k.a reranker) models:\n",
    "\n",
    "- Calculates a similarity score given pairs of texts.\n",
    "- Generally provides superior performance compared to a Sentence Transformer (a.k.a. bi-encoder) model.\n",
    "- Often slower than a Sentence Transformer model, as it requires computation for each pair rather than each text.\n",
    "- Due to the previous 2 characteristics, Cross Encoders are often used to re-rank the top-k results from a Sentence Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model_name = '../../cache/officials/ms-marco-MiniLM-L-6-v2'\n",
    "model = CrossEncoder(model_name)\n",
    "\n",
    "# two way pose no difference to model\n",
    "pairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\n",
    "# pairs = [('what is panda?', 'hi'), ('what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.')]\n",
    "scores = model.predict(pairs, show_progress_bar=True)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = '../../cache/officials/ms-marco-MiniLM-L-6-v2'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    scores = model(**inputs).logits\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieval and re-rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data02/hyzhang10/miniconda3/envs/xp-nlp/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "No sentence-transformers model found with name ../../cache/officials/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n",
      "/data02/hyzhang10/miniconda3/envs/xp-nlp/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passages: 509663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6899d5d9b0344b59f4fca099dbc1170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import gzip\n",
    "import os\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: No GPU found. Please add GPU to your notebook\")\n",
    "    \n",
    "def search(query, bi_encoder, cross_encoder):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    ##### Retreving / Semantic Search #####\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    question_embedding = question_embedding.cuda()\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]  # Get the hits for the first query, or the only query\n",
    "    # hits now has score, corpus_id. \n",
    "\n",
    "    ##### Re-Ranking #####\n",
    "    # Now, score all retrieved passages with the cross_encoder\n",
    "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "    # cross_scores is just a one-dim array, meaning the corresponding score for each paired data. \n",
    "\n",
    "    # add the cross-encoder scores into hits. So that hits has scores of both retrieval and re-rank.\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    # Output of top-5 hits from bi-encoder\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Bi-Encoder Retrieval hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    # Output of top-5 hits from re-ranker\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "\n",
    "#We use the Bi-Encoder to encode all passages, so that we can use it with semantic search\n",
    "bi_encoder = SentenceTransformer('../../cache/officials/all-MiniLM-L6-v2')\n",
    "bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "top_k = 32                          #Number of passages we want to retrieve with the bi-encoder\n",
    "#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('../../cache/officials/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# As dataset, we use Simple English Wikipedia. Compared to the full English wikipedia, it has only\n",
    "# about 170k articles. We split these articles into paragraphs and encode them with the bi-encoder\n",
    "\n",
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "if not os.path.exists(wikipedia_filepath):\n",
    "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n",
    "\n",
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        # add all paragraphs\n",
    "        passages += data['paragraphs']\n",
    "        \n",
    "        # add only the first to save time\n",
    "        # passages.append(data['paragraphs'][0])\n",
    "\n",
    "print(\"Passages:\", len(passages))\n",
    "\n",
    "# We encode all passages into our vector space. This takes about 5 minutes (depends on your GPU speed)\n",
    "corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: What is the capital of the United States?\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Bi-Encoder Retrieval hits\n",
      "\t0.708\tUnited States Capitol.\n",
      "\t0.623\tIts capital and largest city is Boston. It is on the east coast of the United States. It is next to the Atlantic Ocean and the states of Rhode Island, Connecticut, New York, Vermont, and New Hampshire. The word \"Massachusetts\" comes from Native American language. It means \"place with hills.\"\n",
      "\t0.605\tThe continental United States is the area of the United States of America that is located in the continent of North America. It includes 49 of the 50 states (48 of which are located south of Canada and north of Mexico, known as the \"lower 48 states\", the other being Alaska) and the District of Columbia, which contains the federal capital, Washington, D.C. The only state which is not part of this is Hawaii (as they are islands in the Pacific Ocean and not part of North America).\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Cross-Encoder Re-ranker hits\n",
      "\t8.906\tWashington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district. The President of the USA and many major national government offices are in the territory. This makes it the political center of the United States of America.\n",
      "\t7.323\tThe first capital city of the United States was New York City. At this time, Congress met in City Hall (Federal Hall) from 1785 to 1790. When the capital was moved to Philadelphia, Pennsylvania, from 1790 to 1800, the Philadelphia County Building (Congress Hall) became the capitol. In 1800, the capital moved again to Washington, D.C., and a new capitol building was built.\n",
      "\t4.650\tIn 1783, it was the capital of the United States for a few months.\n"
     ]
    }
   ],
   "source": [
    "search(\"What is the capital of the United States?\", bi_encoder, cross_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: How long do cats live?\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Bi-Encoder Retrieval hits\n",
      "\t0.724\tReliable information on the lifespans of house cats is hard to find. However, research has been done to get an estimate (an educated guess) on how long cats usually live. Cats usually live for 13 to 20 years. Sometimes cats can live for 22 to 30 years but there are claims of cats dying at ages of more than 30 years old.\n",
      "\t0.711\tThe life expectancy of an indoor cat is around 17 years, but the life expectancy of outdoor cats is 5.6 years.\n",
      "\t0.663\tVery little is known about how long they live. Some reports say \"30 years or more\", while others say \"50 years or more\".\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Cross-Encoder Re-ranker hits\n",
      "\t10.431\tReliable information on the lifespans of house cats is hard to find. However, research has been done to get an estimate (an educated guess) on how long cats usually live. Cats usually live for 13 to 20 years. Sometimes cats can live for 22 to 30 years but there are claims of cats dying at ages of more than 30 years old.\n",
      "\t8.532\tThe life expectancy of an indoor cat is around 17 years, but the life expectancy of outdoor cats is 5.6 years.\n",
      "\t8.132\tPeople sometimes guess how long a cat will live by comparing it to how long a human usually lives. You can estimate a cat's age in \"cat years\" by multiplying the cat's age in normal years by 7. A better guess for cat years used by veterinarians is: the cat's age times 4, then plus 16. You can use this math for cats who are at least 2 years old.\n"
     ]
    }
   ],
   "source": [
    "search(\"How long do cats live?\", bi_encoder, cross_encoder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xp-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
